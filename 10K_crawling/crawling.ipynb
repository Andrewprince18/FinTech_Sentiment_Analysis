{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import time\n",
    "\n",
    "SEARCHAPIENDPOINT = \"https://efts.sec.gov/LATEST/search-index/\" \n",
    "ARCHIVESBASEURL = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "\n",
    "#SEARCHAPIENDPOINT = \"https://data.sec.gov/submissions/\"\n",
    "#ARCHIVESBASEURL = \"https://www.sec.gov/Archives/edgar/daily-index/\"\n",
    "\n",
    "SLEEPTIME = 0.2\n",
    "MAXRETRIES = 10\n",
    "DATE_FORMAT_TOKENS = \"%Y-%m-%d\"\n",
    "AFTER_DATE = date(2000, 1, 1)\n",
    "BEFORE_DATE = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step\n",
    "obtain the URLs of the filing we want to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "retries = Retry(\n",
    "    total=MAXRETRIES,\n",
    "    backoff_factor=SLEEPTIME,\n",
    "    status_forcelist=[403, 500, 502, 503, 504],\n",
    ")\n",
    "\n",
    "ROOT_SAVE_FOLDER_NAME = \"test\"\n",
    "FILING_FULL_SUBMISSION_FILENAME = \"full-submission.txt\"\n",
    "FILING_DETAILS_FILENAME_STEM = \"filing-details\"\n",
    "\n",
    "FilingMetadata = namedtuple(\n",
    "    \"FilingMetadata\",\n",
    "    [ \"cik\",\n",
    "      \"file_date\",\n",
    "      \"period_end\",\n",
    "      \"accession_number\",\n",
    "      \"full_submission_url\",\n",
    "      \"filing_details_url\",\n",
    "      \"filing_details_filename\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def form_request(\n",
    "    ticker: str,\n",
    "    filing_types: List[str],\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    start_index: int,\n",
    "    query: str,\n",
    ") -> dict:\n",
    "    request = {\n",
    "        \"dateRange\": \"custom\",\n",
    "        \"startdt\": start_date,\n",
    "        \"enddt\": end_date,\n",
    "        \"entityName\": ticker,\n",
    "        \"forms\": filing_types,\n",
    "        \"from\": start_index,\n",
    "        \"q\": query,\n",
    "    }\n",
    "    return request\n",
    "\n",
    "def user_agent() -> str:\n",
    "    return f\"{fake.first_name()} {fake.last_name()} {fake.email()}\"\n",
    "\n",
    "def filing_metadata(hit: dict) -> FilingMetadata:\n",
    "    accession_number, filing_details_filename = hit[\"_id\"].split(\":\", 1)\n",
    "    cik = hit[\"_source\"][\"ciks\"][-1]\n",
    "    file_date= hit[\"_source\"][\"file_date\"]\n",
    "    period_ending=hit[\"_source\"][\"period_ending\"]\n",
    "    accession_number_no_dashes = accession_number.replace(\"-\", \"\", 2)\n",
    "    submission_base_url = (\n",
    "        f\"{ARCHIVESBASEURL}/{cik}/{accession_number_no_dashes}\"\n",
    "    )\n",
    "    full_submission_url = f\"{submission_base_url}/{accession_number}.txt\"\n",
    "    filing_details_url = f\"{submission_base_url}/{filing_details_filename}\"\n",
    "    filing_details_filename_extension = Path(filing_details_filename).suffix.replace(\n",
    "        \"htm\", \"html\"\n",
    "    )\n",
    "    filing_details_filename = (\n",
    "        f\"{FILING_DETAILS_FILENAME_STEM}{filing_details_filename_extension}\"\n",
    "    )\n",
    " \n",
    "    return FilingMetadata(\n",
    "        cik,\n",
    "        file_date,\n",
    "        period_ending,\n",
    "        accession_number=accession_number,\n",
    "        full_submission_url=full_submission_url,\n",
    "        filing_details_url=filing_details_url,\n",
    "        filing_details_filename=filing_details_filename,\n",
    "    )\n",
    " \n",
    "filings_to_download: List[FilingMetadata] = []\n",
    " \n",
    "def get_filing_urls(    #obtain the filing urls we want to download\n",
    "    filing_type: str,\n",
    "    ticker: str,\n",
    "    num_filings_to_download: int,\n",
    "    after_date: str,\n",
    "    before_date: str,\n",
    "    include_amends: bool,\n",
    "    query: str = \"\",\n",
    ") -> List[FilingMetadata]:\n",
    "\n",
    "    start_index = 0\n",
    "    # create a session to connect to the API\n",
    "    client = requests.Session()   \n",
    "    client.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    try:\n",
    "            while len(filings_to_download)<num_filings_to_download:\n",
    "                payload = form_request(\n",
    "                    ticker,\n",
    "                    [filing_type],\n",
    "                    after_date,\n",
    "                    before_date,\n",
    "                    start_index,\n",
    "                    query,\n",
    "                )\n",
    "                headers = {\n",
    "                    \"User-Agent\": user_agent(),\n",
    "                    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                    \"Host\": \"efts.sec.gov\",\n",
    "                }\n",
    "                resp = client.post(  # send the request and data to the server\n",
    "                SEARCHAPIENDPOINT, json=payload, headers=headers # 為了要精確的搜尋，所以要指定 1.API的網址 2.要傳送的資料 3.要傳送的標頭 \n",
    "                )\n",
    "                resp.raise_for_status()\n",
    "                queryresults = resp.json() # return the json object of the result\n",
    "                print(queryresults) # the result is stored in the \"queryresults\" variable\n",
    "\n",
    "                queryhits = queryresults[\"hits\"][\"hits\"]\n",
    "                print(queryhits)\n",
    "\n",
    "                if not queryhits:\n",
    "                    break\n",
    "\n",
    "                for hit in queryhits:\n",
    "                    filing_type = hit[\"_source\"][\"file_type\"]\n",
    "\n",
    "                    is_amend = filing_type[-2:] == \"/A\"\n",
    "                    if not include_amends and is_amend:\n",
    "                        continue\n",
    "\n",
    "                    if not is_amend and filing_type != filing_type:\n",
    "                        continue\n",
    "\n",
    "                metadata = filing_metadata(hit)  # the filing we want to download is stored in the \"metadata\" variable\n",
    "                filings_to_download.append(metadata)\n",
    "\n",
    "                if len(filings_to_download) == num_filings_to_download:\n",
    "                    return filings_to_download\n",
    "\n",
    "\n",
    "            query_size = queryresults[\"query\"][\"size\"]\n",
    "            start_index += query_size\n",
    "\n",
    "            time.sleep(SLEEPTIME)\n",
    "    finally:\n",
    "            client.close()\n",
    "\n",
    "            return filings_to_download\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return URLs\n",
    "for all 10-K filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_filing_urls(\n",
    "    \"10-K\",\n",
    "    \"AAPL\",\n",
    "    999999999999999999999999,\n",
    "    \"2012-01-01\",\n",
    "    \"2022-01-01\",\n",
    "    include_amends=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "def resolve_relative_urls(filing_text: str, download_url: str) -> str:\n",
    "     soup = BeautifulSoup(filing_text, \"lxml\")\n",
    "     base_url = f\"{download_url.rsplit('/', 1)[0]}/\"\n",
    "\n",
    "     for url in soup.find_all(\"a\", href=True):\n",
    "         if url[\"href\"].startswith(\"#\") or url[\"href\"].startswith(\"http\"):\n",
    "            continue\n",
    "         url[\"href\"] = urljoin(base_url, url[\"href\"])\n",
    "\n",
    "     for image in soup.find_all(\"img\", src=True):\n",
    "         image[\"src\"] = urljoin(base_url, image[\"src\"])\n",
    "\n",
    "     if soup.original_encoding is None:\n",
    "         return soup\n",
    "\n",
    "     return soup.encode(soup.original_encoding)\n",
    "\n",
    "def download_filings(   # download the 10-K filings\n",
    "    download_folder: Path,\n",
    "    ticker: str,\n",
    "    filing_type: str,\n",
    "    num_filings_to_download: int,\n",
    "    after_date: str,\n",
    "    before_date: str,\n",
    "    include_filing_details: bool,\n",
    ") -> None:\n",
    "    client = requests.Session()\n",
    "    client.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    client.mount(\"https://\", HTTPAdapter(max_retries=retries)) # call another function to download the filing\n",
    "    try:\n",
    "        for filing in filings_to_download: # the URLs is stored in the \"filings_to_download\" list\n",
    "            try:\n",
    "                download(\n",
    "                    client,\n",
    "                    download_folder,\n",
    "                    ticker,\n",
    "                    filing.accession_number,\n",
    "                    filing_type,\n",
    "                    filing.full_submission_url,\n",
    "                    FILING_FULL_SUBMISSION_FILENAME, # download TXT filings , HTML filings depending on the format of the filing\n",
    "                )\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                print(\n",
    "                    \"Skipping full submission download for \"\n",
    "                    f\"'{filing.accession_number}' due to network error: {e}.\"\n",
    "                )\n",
    "\n",
    "            if include_filing_details:\n",
    "                try:\n",
    "                    download(\n",
    "                        client,\n",
    "                        download_folder,\n",
    "                        ticker,\n",
    "                        filing.accession_number,\n",
    "                        filing_type,\n",
    "                        filing.filing_details_url,\n",
    "                        filing.filing_details_filename,\n",
    "                        resolve_urls=True,\n",
    "                    )   \n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    print(\n",
    "                        f\"Skipping filing detail download for \"\n",
    "                        f\"'{filing.accession_number}' due to network error: {e}.\"\n",
    "                    )\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def download(\n",
    "    client: requests.Session,\n",
    "    download_folder: Path,\n",
    "    ticker: str,\n",
    "    accession_number: str,\n",
    "    filing_type: str,\n",
    "    download_url: str,\n",
    "    save_filename: str,\n",
    "    *,\n",
    "    resolve_urls: bool = False,\n",
    ") -> None:\n",
    "     headers = {\n",
    "         \"User-Agent\": user_agent(),\n",
    "         \"Accept-Encoding\": \"gzip, deflate\",\n",
    "         \"Host\": \"www.sec.gov\",\n",
    "    }\n",
    "     resp = client.get(download_url, headers=headers) # get the response and the status from the URLs\n",
    "     resp.raise_for_status()\n",
    "     filing_text = resp.content  # return the content of the response in bytes\n",
    "\n",
    "     if resolve_urls and Path(save_filename).suffix == \".html\":\n",
    "       filing_text = resolve_relative_urls(filing_text, download_url)\n",
    "\n",
    "     save_path = (\n",
    "        download_folder\n",
    "        / ROOT_SAVE_FOLDER_NAME\n",
    "        / ticker\n",
    "        / filing_type\n",
    "        / accession_number\n",
    "        / save_filename\n",
    "    )\n",
    "     save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "     save_path.write_bytes(filing_text)\n",
    "\n",
    "     time.sleep(SLEEPTIME)\n",
    "\n",
    "cwd = Path.cwd()\n",
    "downloader=cwd/\"/Users/andrewhsu/Documents/fintech_10_K\"\n",
    "download_filings(\n",
    "    downloader,\n",
    "    \"AAPL\",\n",
    "    \"10-K\",\n",
    "    10,\n",
    "    \"2012-01-01\",\n",
    "    \"2023-01-01\",\n",
    "    include_filing_details=\"TRUE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
